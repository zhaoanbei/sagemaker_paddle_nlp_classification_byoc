{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33a9a9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user has root access.\n",
      "nvidia-docker2 already installed. We are good to go!\n",
      "0f22683c1dbf029997203ccf78610a2ae90162d8bb62728e3ec816ae2ebe029b\n",
      "SageMaker instance routing for Docker is ok. We are good to go!\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "sudo chmod +777 utils/setup.sh\n",
    "./utils/setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb05e80f",
   "metadata": {},
   "source": [
    "# docker单元测试\n",
    "\n",
    "1. 修改docker sysconfig更改image存放路径\n",
    "sudo vim /etc/sysconfig/docker\n",
    "OPTIONS=\"--selinux-enabled -g /home/ec2-user/SageMaker/docker\"\n",
    "\n",
    "sudo service docker restart\n",
    "\n",
    "2. nvidia-smi 查看cuda版本\n",
    "\n",
    "https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/docker/linux-docker.html\n",
    "\n",
    "nvidia-docker pull registry.baidubce.com/paddlepaddle/paddle:2.4.1-gpu-cuda11.2-cudnn8.2-trt8.0\n",
    "nvidia-docker run --name paddle -p 8080:8080 -it -v $PWD:/paddle registry.baidubce.com/paddlepaddle/paddle:2.4.1-gpu-cuda11.2-cudnn8.2-trt8.0 /bin/bash \n",
    "\n",
    "3. 运行\n",
    "\n",
    "mkdir /opt/program/\n",
    "cd /opt/program/\n",
    "pip3 install -i https://mirror.baidu.com/pypi/simple --upgrade pip\n",
    "pip3 install -i https://mirror.baidu.com/pypi/simple networkx==2.3 flask gevent gunicorn boto3\n",
    "pip3 install -i https://mirror.baidu.com/pypi/simple paddlenlp onnx onnxconverter_common onnxruntime-gpu nvgpu\n",
    "apt-get -y update && apt-get install -y --no-install-recommends          wget          nginx          ca-certificates     && rm -rf /var/lib/apt/lists/*\n",
    "./serve \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a792834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m ================ INVOCATIONS =================\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m <<<< flask.request.content_type application/json\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m gpu:0\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m \u001b[32m[2023-01-17 07:52:01,977] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load '/opt/program/'.\u001b[0m\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m \u001b[32m[2023-01-17 07:52:01,977] [    INFO]\u001b[0m - loading configuration file /opt/program/config.json\u001b[0m\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m \u001b[32m[2023-01-17 07:52:01,978] [    INFO]\u001b[0m - Model config ErnieConfig {\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"architectures\": [\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m     \"ErnieForSequenceClassification\"\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   ],\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"attention_probs_dropout_prob\": 0.1,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"enable_recompute\": false,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"fuse\": false,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"hidden_act\": \"gelu\",\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"hidden_dropout_prob\": 0.1,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"hidden_size\": 768,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"intermediate_size\": 3072,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"layer_norm_eps\": 1e-12,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"max_position_embeddings\": 2048,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"model_type\": \"ernie\",\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"num_attention_heads\": 12,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"num_hidden_layers\": 6,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"pad_token_id\": 0,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"paddlenlp_version\": null,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"pool_act\": \"tanh\",\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"task_id\": 0,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"task_type_vocab_size\": 16,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"type_vocab_size\": 4,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"use_task_id\": true,\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m   \"vocab_size\": 40000\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m }\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m \u001b[0m\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m W0117 07:52:04.058017    15 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.0, Runtime API Version: 10.2\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m W0117 07:52:04.066195    15 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m \u001b[32m[2023-01-17 07:52:04,650] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing ErnieForSequenceClassification.\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m \u001b[0m\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m \u001b[32m[2023-01-17 07:52:04,650] [    INFO]\u001b[0m - All the weights of ErnieForSequenceClassification were initialized from the model checkpoint at /opt/program/.\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m If your task is similar to the task the model of the checkpoint was trained on, you can already use ErnieForSequenceClassification for predictions without further training.\u001b[0m\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m \u001b[32m[2023-01-17 07:52:04,652] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load '/opt/program/'.\u001b[0m\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m 172.19.0.1 - - [17/Jan/2023:07:52:06 +0000] \"POST /invocations HTTP/1.1\" 200 3 \"-\" \"python-requests/2.26.0\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#locally\n",
    "url='http://localhost:8080/invocations'\n",
    "texts = ['sagemaker-kwm-new','债务人: cksdafgsfgsjhkvgdkjsV 破产管理人:kjLFHk;fhs;dhsd;f']\n",
    "payload = json.dumps(texts)\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "r = requests.post(url,data=payload,headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "981f6129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [502]>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ecaa8b",
   "metadata": {},
   "source": [
    "# sagemaker集成测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53a36392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cn-northwest-1\n",
      "969422986683.dkr.ecr.cn-northwest-1.amazonaws.com.cn/kwm-new:latest\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  776.3MB\n",
      "Step 1/15 : FROM registry.baidubce.com/paddlepaddle/paddle:2.4.1-gpu-cuda10.2-cudnn7.6-trt7.0\n",
      " ---> 98e68d139b09\n",
      "Step 2/15 : ENV LANG=en_US.utf8\n",
      " ---> Using cache\n",
      " ---> cbe993f09e08\n",
      "Step 3/15 : ENV LANG=C.UTF-8\n",
      " ---> Using cache\n",
      " ---> 6da2cfdf8e9e\n",
      "Step 4/15 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 63d4ee36f691\n",
      "Step 5/15 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> 8e8b06a09a44\n",
      "Step 6/15 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 1e5f20be05f0\n",
      "Step 7/15 : RUN mkdir /opt/program/\n",
      " ---> Using cache\n",
      " ---> e2c343ce2f64\n",
      "Step 8/15 : RUN pip3 install -i https://mirror.baidu.com/pypi/simple networkx==2.3 flask gevent gunicorn boto3\n",
      " ---> Using cache\n",
      " ---> 94572a88459d\n",
      "Step 9/15 : RUN pip3 install -i https://mirror.baidu.com/pypi/simple paddlenlp onnx onnxconverter_common onnxruntime-gpu nvgpu\n",
      " ---> Using cache\n",
      " ---> d0c1d38c417b\n",
      "Step 10/15 : RUN apt-get -y update && apt-get install -y --no-install-recommends          wget          nginx          ca-certificates     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 660ae893358b\n",
      "Step 11/15 : RUN ln -sf /dev/stdout /var/log/nginx/access.log\n",
      " ---> Using cache\n",
      " ---> b16164af76f1\n",
      "Step 12/15 : RUN ln -sf /dev/stderr /var/log/nginx/error.log\n",
      " ---> Using cache\n",
      " ---> 5b793052e052\n",
      "Step 13/15 : COPY model /opt/program\n",
      " ---> f380790e97cb\n",
      "Step 14/15 : RUN ls /opt/program\n",
      " ---> Running in 2a62664fbda7\n",
      "__pycache__\n",
      "code\n",
      "config.json\n",
      "infer_gpu.py\n",
      "inference.pdiparams\n",
      "inference.pdiparams.info\n",
      "inference.pdmodel\n",
      "model.py\n",
      "model_config.json\n",
      "model_state.pdparams\n",
      "nginx.conf\n",
      "predictor.py\n",
      "serve\n",
      "setting.txt\n",
      "special_tokens_map.json\n",
      "tokenizer_config.json\n",
      "train\n",
      "uie_predictor.py\n",
      "vocab.txt\n",
      "wsgi.py\n",
      "Removing intermediate container 2a62664fbda7\n",
      " ---> 89a3d45d185d\n",
      "Step 15/15 : WORKDIR /opt/program\n",
      " ---> Running in 83c67a04bc3a\n",
      "Removing intermediate container 83c67a04bc3a\n",
      " ---> 46c3fa4a9082\n",
      "Successfully built 46c3fa4a9082\n",
      "Successfully tagged kwm-new:latest\n",
      "The push refers to repository [969422986683.dkr.ecr.cn-northwest-1.amazonaws.com.cn/kwm-new]\n",
      "\n",
      "\u001b[1Bd3bb57a9: Preparing \n",
      "\u001b[1Bf3935c8a: Preparing \n",
      "\u001b[1Bfeb63f66: Preparing \n",
      "\u001b[1B20945bdf: Preparing \n",
      "\u001b[1B53675f29: Preparing \n",
      "\u001b[1B8b907837: Preparing \n",
      "\u001b[1B2fa9b231: Preparing \n",
      "\u001b[1Be929ce1c: Preparing \n",
      "\u001b[1B28219feb: Preparing \n",
      "\u001b[1Bbad9f62d: Preparing \n",
      "\u001b[1Bfe995f7f: Preparing \n",
      "\u001b[1Bb0f07edf: Preparing \n",
      "\u001b[1Bb6c272f9: Preparing \n",
      "\u001b[1B51de49a5: Preparing \n",
      "\u001b[1B4a091f5c: Preparing \n",
      "\u001b[1B9f286f50: Preparing \n",
      "\u001b[1B7812cc7b: Preparing \n",
      "\u001b[1Bbf970aee: Preparing \n",
      "\u001b[1B12cac86f: Preparing \n",
      "\u001b[1Bdc8ec3c9: Preparing \n",
      "\u001b[1Bc2aee2b6: Preparing \n",
      "\u001b[1B274a605e: Preparing \n",
      "\u001b[1B90a2fa09: Preparing \n",
      "\u001b[1B49dc8918: Preparing \n",
      "\u001b[1B6251f1cf: Preparing \n",
      "\u001b[1B9ff386da: Preparing \n",
      "\u001b[1Be2ff3b98: Preparing \n",
      "\u001b[1B5cec4b65: Preparing \n",
      "\u001b[1Bf1532195: Preparing \n",
      "\u001b[1Be3e258bc: Preparing \n",
      "\u001b[1B276cc332: Preparing \n",
      "\u001b[1B88a4511a: Preparing \n",
      "\u001b[1Bd190f4ea: Preparing \n",
      "\u001b[1B459eebf3: Preparing \n",
      "\u001b[1Bff9edd85: Preparing \n",
      "\u001b[1B53511f39: Preparing \n",
      "\u001b[1B71bdd65d: Preparing \n",
      "\u001b[1B3da030f5: Preparing \n",
      "\u001b[1B7895ed45: Preparing \n",
      "\u001b[1B48c4f2a7: Preparing \n",
      "\u001b[1Bcf40d5fc: Preparing \n",
      "\u001b[2Bcf40d5fc: Waiting g \n",
      "\u001b[1B09412b51: Preparing \n",
      "\u001b[1B03bf5ab9: Preparing \n",
      "\u001b[4B85fec936: Waiting g \n",
      "\u001b[1B22ca4394: Preparing \n",
      "\u001b[1B0fc7f3e9: Preparing \n",
      "\u001b[6B09412b51: Waiting g \n",
      "\u001b[1B8cd0c691: Preparing \n",
      "\u001b[7B03bf5ab9: Waiting g \n",
      "\u001b[1Bf023f765: Preparing \n",
      "\u001b[1B9ab1eb50: Preparing \n",
      "\u001b[9B8fb33871: Waiting g \n",
      "\u001b[9B22ca4394: Waiting g \n",
      "\u001b[1B803d9a98: Preparing \n",
      "\u001b[1B9b6c9051: Preparing \n",
      "\u001b[11Bfc7f3e9: Waiting g \n",
      "\u001b[1B19626b20: Preparing \n",
      "\u001b[1Bc8292d9b: Preparing \n",
      "\u001b[13Bb785540: Waiting g \n",
      "\u001b[61B3bb57a9: Pushed   775.6MB/775.6MB0A\u001b[2K\u001b[44A\u001b[2K\u001b[36A\u001b[2K\u001b[31A\u001b[2K\u001b[26A\u001b[2K\u001b[18A\u001b[2K\u001b[12A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2K\u001b[61A\u001b[2Klatest: digest: sha256:fe3bb5c4b88b7c8d705b2ff0e4cf7650df54a30427d5211c8848b3347d8bdf16 size: 13124\n",
      "Attaching to 065jt82yko-algo-1-wes65\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m Starting the inference server with 4 workers.\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m [2023-01-17 07:51:54 +0000] [9] [INFO] Starting gunicorn 20.1.0\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m [2023-01-17 07:51:54 +0000] [9] [INFO] Listening at: unix:/tmp/gunicorn.sock (9)\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m [2023-01-17 07:51:54 +0000] [9] [INFO] Using worker: gevent\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m [2023-01-17 07:51:54 +0000] [13] [INFO] Booting worker with pid: 13\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m [2023-01-17 07:51:54 +0000] [14] [INFO] Booting worker with pid: 14\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m [2023-01-17 07:51:54 +0000] [15] [INFO] Booting worker with pid: 15\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m [2023-01-17 07:51:54 +0000] [16] [INFO] Booting worker with pid: 16\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m <<< files under opt/program ['predictor.py', 'inference.pdiparams', 'model_state.pdparams', 'vocab.txt', 'model.py', 'nginx.conf', 'inference.pdmodel', 'special_tokens_map.json', 'infer_gpu.py', '.ipynb_checkpoints', 'config.json', 'tokenizer_config.json', '__pycache__', 'serve', 'code', 'model_config.json', 'inference.pdiparams.info', 'wsgi.py', 'train', 'uie_predictor.py', 'setting.txt']\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m start!!!!\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m <<< files under opt/program ['predictor.py', 'inference.pdiparams', 'model_state.pdparams', 'vocab.txt', 'model.py', 'nginx.conf', 'inference.pdmodel', 'special_tokens_map.json', 'infer_gpu.py', '.ipynb_checkpoints', 'config.json', 'tokenizer_config.json', '__pycache__', 'serve', 'code', 'model_config.json', 'inference.pdiparams.info', 'wsgi.py', 'train', 'uie_predictor.py', 'setting.txt']\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m start!!!!\n",
      "!\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m 172.19.0.1 - - [17/Jan/2023:07:51:58 +0000] \"GET /ping HTTP/1.1\" 200 22 \"-\" \"python-urllib3/1.26.8\"\n",
      "CPU times: user 1.2 s, sys: 248 ms, total: 1.45 s\n",
      "Wall time: 1min 4s\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m <<< files under opt/program ['predictor.py', 'inference.pdiparams', 'model_state.pdparams', 'vocab.txt', 'model.py', 'nginx.conf', 'inference.pdmodel', 'special_tokens_map.json', 'infer_gpu.py', '.ipynb_checkpoints', 'config.json', 'tokenizer_config.json', '__pycache__', 'serve', 'code', 'model_config.json', 'inference.pdiparams.info', 'wsgi.py', 'train', 'uie_predictor.py', 'setting.txt']\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m start!!!!\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m <<< files under opt/program ['predictor.py', 'inference.pdiparams', 'model_state.pdparams', 'vocab.txt', 'model.py', 'nginx.conf', 'inference.pdmodel', 'special_tokens_map.json', 'infer_gpu.py', '.ipynb_checkpoints', 'config.json', 'tokenizer_config.json', '__pycache__', 'serve', 'code', 'model_config.json', 'inference.pdiparams.info', 'wsgi.py', 'train', 'uie_predictor.py', 'setting.txt']\n",
      "\u001b[36m065jt82yko-algo-1-wes65 |\u001b[0m start!!!!\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!sh build_and_push.sh kwm-new\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "from datetime import datetime\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model import Model\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sage = boto3.Session().client(service_name='sagemaker') \n",
    "\n",
    "container = f'969422986683.dkr.ecr.cn-northwest-1.amazonaws.com.cn/kwm-new:latest'\n",
    "\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "model = Model(image_uri=container, \n",
    "              model_data=\"file://file.tar.gz\",\n",
    "              role=sagemaker_role)\n",
    "\n",
    "resource_name = \"kwm-new-{}\"\n",
    "endpoint_name = resource_name.format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "\n",
    "initial_instance_count=1\n",
    "instance_type='ml.g4dn.xlarge'\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=initial_instance_count,\n",
    "    instance_type='local_gpu',\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4467f96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!CPU times: user 638 ms, sys: 28.4 ms, total: 667 ms\n",
      "Wall time: 8min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# !sh build_and_push.sh kwm-new\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "from datetime import datetime\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model import Model\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sage = boto3.Session().client(service_name='sagemaker') \n",
    "\n",
    "container = f'969422986683.dkr.ecr.cn-northwest-1.amazonaws.com.cn/kwm-new:latest'\n",
    "\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "model = Model(image_uri=container, \n",
    "              role=sagemaker_role)\n",
    "\n",
    "resource_name = \"kwm-new-{}\"\n",
    "endpoint_name = resource_name.format(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "\n",
    "initial_instance_count=1\n",
    "instance_type='ml.g4dn.xlarge'\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=initial_instance_count,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c257f4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs:  [0]\n",
      "time: 11.58616590499878\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import sagemaker\n",
    "endpoint_name = 'kwm-new-2023-01-17-07-59-23'  \n",
    "predictor = sagemaker.predictor.Predictor(endpoint_name=endpoint_name)\n",
    "\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "texts = ['sagemaker-kwm-new','被告吉林紫鑫药业股份有限公司于本判决生效后十日内给付原告江苏启安建设集团有限公司工程款1，266，485.20元']\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "outputs = predictor.predict(texts)\n",
    "end = time.time()\n",
    "print('outputs: ', outputs)\n",
    "print('time:', end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "778cb0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d67e437b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs:  [{'法院': [{'text': '北京市海淀区人民法院', 'start': 1, 'end': 11, 'probability': 0.9251759648323059}], '原告': [{'text': '张三', 'start': 36, 'end': 38, 'probability': 0.9924564361572266, 'relations': {'委托代理人': [{'text': '李四', 'start': 45, 'end': 47, 'probability': 0.698268473148346}]}}], '被告': [{'text': 'B公司', 'start': 65, 'end': 68, 'probability': 0.804964005947113, 'relations': {'委托代理人': [{'text': '赵六', 'start': 91, 'end': 93, 'probability': 0.6848900318145752}]}}]}]\n",
      "time: 3.469820737838745\n"
     ]
    }
   ],
   "source": [
    "# !cp model/code/requirements_cpu.txt model/code/requirements.txt\n",
    "# !cd model && tar -czvf ../model-inference-cpu.tar.gz *\n",
    "# !aws s3 cp model-inference-cpu.tar.gz s3://sagemaker-cn-northwest-1-969422986683/uie/model-inference-cpu.tar.gz\n",
    "    \n",
    "import os\n",
    "import sagemaker\n",
    "import time\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "instance_type = 'ml.m5.4xlarge'\n",
    "\n",
    "model_data='s3://sagemaker-cn-northwest-1-969422986683/uie/model-inference-cpu.tar.gz'\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=model_data, role=role,\n",
    "                             entry_point='infer_cpu.py', framework_version='1.9.0', py_version='py38', model_server_workers=1)\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type=instance_type, initial_instance_count=1)\n",
    "\n",
    "end = time.time()\n",
    "print('time:', end-start) #10分钟左右\n",
    "\n",
    "# # !pip install -i https://opentuna.cn/pypi/web/simple/ numpy==1.21.6\n",
    "# import os\n",
    "# import json\n",
    "\n",
    "# import argparse\n",
    "# from pprint import pprint\n",
    "\n",
    "# import paddle\n",
    "# import sys\n",
    "# sys.path.append('./model/code') #folder which contains model, snn etc.,\n",
    "# import time\n",
    "\n",
    "# # from uie_predictor import UIEPredictor\n",
    "# import numpy as np\n",
    "# import paddle.nn.functional as F\n",
    "# from paddlenlp.utils.log import logger\n",
    "# from paddle.io import DataLoader, BatchSampler\n",
    "# from paddlenlp.data import DataCollatorWithPadding\n",
    "# from paddlenlp.datasets import load_dataset\n",
    "# from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# import re\n",
    "\n",
    "# paddle.device.set_device(\"gpu\")\n",
    "\n",
    "# class NpEncoder(json.JSONEncoder):\n",
    "#     def default(self, obj):\n",
    "#         if isinstance(obj, np.integer):\n",
    "#             return int(obj)\n",
    "#         if isinstance(obj, np.floating):\n",
    "#             return float(obj)\n",
    "#         if isinstance(obj, np.ndarray):\n",
    "#             return obj.tolist()\n",
    "#         return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "# def input_fn(request_body, request_content_type):\n",
    "#     if request_content_type == 'application/json':\n",
    "#         input_data = json.loads(request_body)\n",
    "#         return input_data\n",
    "#     else:\n",
    "#         # Handle other content-types here or raise an Exception\n",
    "#         # if the content type is not supported.  \n",
    "#         return request_body\n",
    "    \n",
    "\n",
    "# def model_fn(model_dir):\n",
    "# #     args = parse_args()\n",
    "# #     args.model_path_prefix = os.path.join(model_dir, 'inference')\n",
    "# # #     args.device = 'cpu'\n",
    "# #     args.device = 'gpu'\n",
    "# #     args.schema = ['债务人', '破产管理人']\n",
    "# #     predictor1 = UIEPredictor(args)\n",
    "    \n",
    "#     params_path = model_dir       # 模型checkpoint文件夹的路径\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(params_path)\n",
    "#     model.eval()\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(params_path)\n",
    "#     return (model,tokenizer)#predictor1,\n",
    "  \n",
    "# def predict_fn(input_data, model):\n",
    "# #     print('predict_fn',model)\n",
    "#     if input_data[0]=='sagemaker-kwm-iue': # model name needs to mapped\n",
    "# #         print('use model sagemaker-kwm-iue')\n",
    "# #         output = model[2].predict([input_data[1]])\n",
    "#         print('wrong id')\n",
    "#         pass\n",
    "\n",
    "#     elif input_data[0]=='sagemaker-kwm-new':# model name needs to mapped\n",
    "#         idx2label = [1, 0]\n",
    "#         tokenizer=model[1]\n",
    "#         new_model = model[0]\n",
    "#         inputs = tokenizer([input_data[1]], max_len_seq=512)\n",
    "#         input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "#         input_ids, token_type_ids = paddle.to_tensor(input_ids), paddle.to_tensor(token_type_ids)\n",
    "#         logits = new_model(input_ids, token_type_ids)\n",
    "#         probs = F.softmax(logits, axis=1)\n",
    "#         idx = paddle.argmax(probs, axis=1).numpy()\n",
    "#         idx = idx.tolist()\n",
    "#         output = [idx2label[i] for i in idx]\n",
    "#     return output\n",
    "\n",
    "# def output_fn(prediction, content_type):\n",
    "#     if content_type == 'application/json':\n",
    "#         return json.dumps(prediction, ensure_ascii=False, cls=NpEncoder)\n",
    "#     return prediction\n",
    "\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "# #     # Required parameters\n",
    "# #     parser.add_argument(\n",
    "# #         \"--model_path_prefix\",\n",
    "# #         type=str,\n",
    "# #         required=True,\n",
    "# #         help=\"The path prefix of inference model to be used.\", )\n",
    "#     parser.add_argument(\n",
    "#         \"--position_prob\",\n",
    "#         default=0.5,\n",
    "#         type=float,\n",
    "#         help=\"Probability threshold for start/end index probabiliry.\", )\n",
    "#     parser.add_argument(\n",
    "#         \"--use_fp16\",\n",
    "#         action='store_true',\n",
    "#         help=\"Whether to use fp16 inference, only takes effect when deploying on gpu.\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--max_seq_len\",\n",
    "#         default=512,\n",
    "#         type=int,\n",
    "#         help=\"The maximum input sequence length. Sequences longer than this will be split automatically.\",\n",
    "#     )\n",
    "    \n",
    "#     parsed, unknown = parser.parse_known_args() # this is an 'internal' method\n",
    "\n",
    "#     for arg in unknown:\n",
    "#         if arg.startswith((\"-\", \"--\")):\n",
    "#             # you can pass any arguments to add_argument\n",
    "#             parser.add_argument(arg, type=str)\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "#     return args\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     model = model_fn('/home/ec2-user/SageMaker/paddlenlp_sagemaker/model/')\n",
    "# #     texts = ['sagemaker-kwm-iue','债务人: cksdafgsfgsjhkvgdkjsV 破产管理人:kjLFHk;fhs;dhsd;f']\n",
    "#     texts = ['sagemaker-kwm-new','查封担保人李凤香的位于农安镇宝塔街西侧富苑小区3栋4单元202室，建筑面积78.14平方米']\n",
    "#     start = time.time()\n",
    "#     result = predict_fn(texts, model)\n",
    "#     result_json = output_fn(result, content_type='application/json')\n",
    "#     end = time.time()\n",
    "\n",
    "#     print('result_json',result_json)\n",
    "#     print('time:', end-start) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "281bb7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"sagemaker-kwm-iue\", \"\\\\u503a\\\\u52a1\\\\u4eba: cksdafgsfgsjhkvgdkjsV \\\\u7834\\\\u4ea7\\\\u7ba1\\\\u7406\\\\u4eba:kjLFHk;fhs;dhsd;f\"]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !tar -czvf ../backup.tar.gz *\n",
    "# !aws s3 cp ../backup.tar.gz s3://sagemaker-cn-northwest-1-969422986683/uie/backup.tar.gz\n",
    "texts = ['sagemaker-kwm-iue','债务人: cksdafgsfgsjhkvgdkjsV 破产管理人:kjLFHk;fhs;dhsd;f']\n",
    "\n",
    "payload = json.dumps(texts)\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "202d6a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98d7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
